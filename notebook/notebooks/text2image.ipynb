{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa3b0e3c",
   "metadata": {},
   "source": [
    "# Build a Text-Image Search Engine in Celestica\n",
    "\n",
    "This notebook illustrates how to build an text-image search engine from scratch using Celestica. Celestica is the most advanced open-source vector database built on top of IPFS/IPLD and supports nearest neighbor embedding search across tens of millions of entries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ef6b1a",
   "metadata": {},
   "source": [
    "### Prepare the data\n",
    "\n",
    "The dataset used in this demo is a subset of the ImageNet dataset (100 classes, 10 images for each class), and the dataset is available via [Github](https://github.com/towhee-io/examples/releases/download/data/reverse_image_search.zip). \n",
    "\n",
    "The dataset is organized as follows:\n",
    "- **train**: directory of candidate images;\n",
    "- **test**: directory of test images;\n",
    "- **reverse_image_search.csv**: a csv file containing an ***id***, ***path***, and ***label*** for each image;\n",
    "\n",
    "Let's take a quick look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf4abc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! curl -L https://github.com/towhee-io/examples/releases/download/data/reverse_image_search.zip -O\n",
    "! unzip -q -o reverse_image_search.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490d1379",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('reverse_image_search.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e98f62",
   "metadata": {},
   "source": [
    "### Create a Celestica Collection\n",
    "\n",
    "Let's first create a `text_image_search` collection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b28770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30aa33c",
   "metadata": {},
   "source": [
    "## Text Image Search\n",
    "\n",
    "In this section, we'll show how to build our text-image search engine using Celestica. The basic idea behind our text-image search is the extract embeddings from images and texts using a deep neural network and compare the embeddings with those stored in Celestica."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee02951",
   "metadata": {},
   "source": [
    "### Generate image and text embeddings with CLIP\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefdd5e5",
   "metadata": {},
   "source": [
    "This operator extracts features for image or text with [CLIP](https://openai.com/blog/clip/) which can generate embeddings for text and image by jointly training an image encoder and text encoder to maximize the cosine similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54927fb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import clip\n",
    "\n",
    "def read_image_rgb(path):\n",
    "    img = cv2.imread(path)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Try with clip_vit_base_patch16\n",
    "# available models = ['RN50', 'RN101', 'RN50x4', 'RN50x16', 'RN50x64', 'ViT-B/32', 'ViT-B/16', 'ViT-L/14', 'ViT-L/14@336px'\n",
    "def clip_image_embedding(img, model_name='ViT-B/32'):\n",
    "    model, preprocess = clip.load(model_name, device='cpu')\n",
    "    img = Image.fromarray(img)\n",
    "    img = preprocess(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = model.encode_image(img)\n",
    "    return features.numpy()\n",
    "\n",
    "def normalize_vector(vec):\n",
    "    return vec / np.linalg.norm(vec)\n",
    "\n",
    "# Execute the pipeline\n",
    "input_path = '../data/teddy.png'\n",
    "img = read_image_rgb(input_path)\n",
    "vec = clip_image_embedding(img)\n",
    "normalized_vec = normalize_vector(vec)\n",
    "\n",
    "print(normalized_vec)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbea7e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def clip_text_embedding(text, model_name='ViT-B/32'):\n",
    "    model, preprocess = clip.load(model_name, device='cpu')\n",
    "    text_tokenized = clip.tokenize([text]).to('cpu')\n",
    "    with torch.no_grad():\n",
    "        features = model.encode_text(text_tokenized)\n",
    "    return features.numpy()\n",
    "\n",
    "def normalize_vector(vec):\n",
    "    return vec / np.linalg.norm(vec)\n",
    "\n",
    "# Execute the pipeline\n",
    "input_text = \"A teddybear on a skateboard in Times Square.\"\n",
    "vec = clip_text_embedding(input_text)\n",
    "normalized_vec = normalize_vector(vec)\n",
    "\n",
    "print(normalized_vec)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61433908",
   "metadata": {},
   "source": [
    "### Load Image Embeddings into Celestica\n",
    "\n",
    "We first extract embeddings from images with `ViT-B/32` model and insert the embeddings into Celestica for indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c70df43",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "import clip\n",
    "from celestica.client import CelesticaClient\n",
    "\n",
    "def read_csv(csv_path, encoding='utf-8-sig'):\n",
    "    with open(csv_path, 'r', encoding=encoding) as f:\n",
    "        data = csv.DictReader(f)\n",
    "        for line in data:\n",
    "            yield int(line['id']), line['path']\n",
    "\n",
    "def read_image_rgb(path):\n",
    "    img = cv2.imread(path)\n",
    "    return cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "def clip_image_embedding(img, model_name='ViT-B/32'):\n",
    "    model, preprocess = clip.load(model_name, device='cpu')\n",
    "    img = Image.fromarray(img)\n",
    "    img = preprocess(img).unsqueeze(0)\n",
    "    with torch.no_grad():\n",
    "        features = model.encode_image(img)\n",
    "    return features.numpy()\n",
    "\n",
    "def normalize_vector(vec):\n",
    "    return vec / np.linalg.norm(vec)\n",
    "\n",
    "def process_csv_file(csv_file):\n",
    "    url = \"celestica:50051\"  # Replace with the address and port of your gRPC server\n",
    "    client = CelesticaClient(url)\n",
    "    \n",
    "    for id, path in read_csv(csv_file):\n",
    "        img = read_image_rgb(path)\n",
    "        vec = clip_image_embedding(img)\n",
    "        normalized_vec = normalize_vector(vec)\n",
    "        #if id == 1:\n",
    "            #print(f\"ID: {id}\")\n",
    "            #print(f\"Vec type: {type(vec)}, Vec value: {vec}\")\n",
    "            #print(f\"Normalized Vec type: {type(normalized_vec)}, Normalized Vec value: {normalized_vec}\")\n",
    "\n",
    "        client.insert(normalized_vec.tolist(), [id])\n",
    "\n",
    "csv_file = 'reverse_image_search.csv'\n",
    "process_csv_file(csv_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b1538a",
   "metadata": {},
   "source": [
    "### Query Matched Images from Celestica"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e04eff7",
   "metadata": {},
   "source": [
    "Now that embeddings for candidate images have been inserted into Celestica, we can query across it for nearest neighbors. Because Celestica only outputs image IDs and distance values, we provide a `read_images` function to get the original image based on IDs and display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02483f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from celestica.client import CelesticaClient\n",
    "from IPython.display import display, Image\n",
    "from PIL import Image as PILImage\n",
    "\n",
    "def read_image(image_ids):\n",
    "    df = pd.read_csv('reverse_image_search.csv')\n",
    "    id_img = df.set_index('id')['path'].to_dict()\n",
    "    imgs = []\n",
    "    for image_id in image_ids:\n",
    "        path = id_img[image_id]\n",
    "        imgs.append(read_image_rgb(path))\n",
    "    return imgs\n",
    "\n",
    "def process_text_query(text):\n",
    "    # Compute text embedding\n",
    "    vec = clip_text_embedding(text)\n",
    "    normalized_vec = normalize_vector(vec)\n",
    "\n",
    "    # Search using Celestica client\n",
    "    url = \"celestica:50051\"  # Replace with the address and port of your gRPC server\n",
    "    client = CelesticaClient(url)\n",
    "    knbn = 5\n",
    "    ef = 10\n",
    "    neighbours = client.search(normalized_vec.tolist(), knbn, ef)\n",
    "\n",
    "    # Get image IDs from search results\n",
    "    image_ids = [neighbour.point_id.index for neighbour in neighbours[0].neighbour]\n",
    "\n",
    "    # Read images\n",
    "    images = read_image(image_ids)\n",
    "    return text, images\n",
    "\n",
    "text1, images1 = process_text_query(\"A white dog\")\n",
    "text2, images2 = process_text_query(\"A black dog\")\n",
    "\n",
    "print(f\"Query: {text1}\")\n",
    "for i, img in enumerate(images1):\n",
    "    display(PILImage.fromarray(img))\n",
    "\n",
    "print(f\"Query: {text2}\")\n",
    "for i, img in enumerate(images2):\n",
    "    display(PILImage.fromarray(img))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "452ee1ccfaba3d4bb36278ef299934b47178eea3ed5a06471ad5391f6c0e0271"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
